{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Object detection and Threshold/Occupancy maker.."
      ],
      "metadata": {
        "id": "Ga3C_jIQzlfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object detection"
      ],
      "metadata": {
        "id": "ljNE2U6N0tJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro6wI3LUxHA7",
        "outputId": "301f0e51-7614-42e6-9e1b-d46e75714ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2024-7-11 Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displayed image saved locally as output_image_0.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model_path = 'best.pt'  # Model\n",
        "\n",
        "# Check if the model file exists\n",
        "if not Path(model_path).exists():\n",
        "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "\n",
        "# Load the model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
        "\n",
        "# Load an image\n",
        "img_path = 'stitched_image_output.png'\n",
        "if not Path(img_path).exists():\n",
        "    raise FileNotFoundError(f\"Image file not found at {img_path}\")\n",
        "\n",
        "# Perform inference / Prediction\n",
        "results = model(img_path)\n",
        "\n",
        "\n",
        "def render_without_labels(results):\n",
        "    for i, (im, pred) in enumerate(zip(results.ims, results.pred)): # Use results.ims instead of results.imgs\n",
        "        if pred is not None:\n",
        "            for *box, conf, cls in reversed(pred):  # xyxy, confidence, class\n",
        "                label = f'{results.names[int(cls)]} {conf:.2f}' if conf > 0 else f'{results.names[int(cls)]}'\n",
        "                # Remove label and confidence display\n",
        "                # Make below comment line executable to get object namings\n",
        "                # cv2.putText(im, label, (int(box[0]), int(box[1]) - 2), 0, 0.6, [225, 255, 255], thickness=1, lineType=cv2.LINE_AA)\n",
        "                cv2.rectangle(im, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), -1)\n",
        "        yield im\n",
        "\n",
        "displayed_image_np = list(render_without_labels(results))\n",
        "\n",
        "# Save the results\n",
        "for i, img_np in enumerate(displayed_image_np):\n",
        "    output_image_path = f'output_image_{i}.png'\n",
        "    cv2.imwrite(output_image_path, img_np)\n",
        "    print(f\"Displayed image saved locally as {output_image_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal png creator"
      ],
      "metadata": {
        "id": "DOPW7a5k0yAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Marked\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to apply thresholding to an image\n",
        "def apply_threshold(image, threshold_value):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresholded_image = cv2.threshold(gray_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_image\n",
        "\n",
        "\n",
        "# Define threshold value (adjust as needed)\n",
        "threshold_value = 150\n",
        "\n",
        "# Apply thresholding to each displayed image and save\n",
        "for i, img_np in enumerate(displayed_image_np):\n",
        "    thresholded_img = apply_threshold(img_np, threshold_value)\n",
        "    output_image_path = f'output_image_{i}_thresholded.png'\n",
        "    cv2.imwrite(output_image_path, thresholded_img)\n",
        "    print(f\"Thresholded image saved locally as {output_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB6LKn2sxc4U",
        "outputId": "b6721a26-fc31-4f24-9417-9c1aa432f9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thresholded image saved locally as output_image_0_thresholded.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Occcupancy grid maker with 512x512 resolution and with .pgm extension"
      ],
      "metadata": {
        "id": "IG_ObIIi02ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to apply thresholding to an image\n",
        "def apply_threshold(image, threshold_value):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresholded_image = cv2.threshold(gray_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_image\n",
        "\n",
        "# Function to resize an image to 512x512 resolution\n",
        "def resize_image(image, width, height):\n",
        "    return cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Assuming you have `displayed_image_np` from your previous code\n",
        "#displayed_image_np = []  # Placeholder for your list of images\n",
        "\n",
        "# Define threshold value (adjust as needed)\n",
        "threshold_value = 150\n",
        "\n",
        "# Desired output resolution\n",
        "output_width = 512\n",
        "output_height = 512\n",
        "\n",
        "# Apply thresholding, resize and save each displayed image\n",
        "for i, img_np in enumerate(displayed_image_np):\n",
        "    # Apply thresholding\n",
        "    thresholded_img = apply_threshold(img_np, threshold_value)\n",
        "\n",
        "    # Resize to 512x512\n",
        "    resized_img = resize_image(thresholded_img, output_width, output_height)\n",
        "\n",
        "    # Save as .pgm file\n",
        "    output_image_path = f'output_image_{i}_thresholded.pgm'\n",
        "    cv2.imwrite(output_image_path, resized_img)\n",
        "    print(f\"Thresholded image saved locally as {output_image_path}\")\n",
        "\n",
        "print(\"All images have been processed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbOGWC6JyccP",
        "outputId": "036b8f28-4e9b-4b60-8605-674cc398c1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thresholded image saved locally as output_image_0_thresholded.pgm\n",
            "All images have been processed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CG5f1qK1pxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}